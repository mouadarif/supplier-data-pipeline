# Quick Start Guide - FAST MODE (5-8x Speedup) üöÄ

## TL;DR - Run This Now

```powershell
# Close Excel first!
python run_fast.py
```

That's it! The script automatically applies all optimizations.

---

## What It Does Automatically

### ‚úÖ Parallel Processing
- Uses all CPU cores (typically 4-16)
- Processes multiple suppliers simultaneously
- **Speedup**: 4-8x depending on CPU

### ‚úÖ Smart Checkpoint Location
- Stores checkpoint in `C:\Users\...\AppData\Local\Temp\`
- Avoids OneDrive file locking
- **Speedup**: 2-3x on file operations

### ‚úÖ Progress Tracking
- Shows: processed count, percentage, rate, ETA
- Example: `[pipeline] processed=500/2440 (20%) | rate=2.5 rows/sec | ETA=12.9 mins`

### ‚úÖ Resume Support
- Can interrupt anytime (Ctrl+C)
- Automatically resumes from checkpoint
- No lost work!

### ‚úÖ Excel Lock Detection
- Checks if Excel has file open
- Warns you before starting
- Prevents cryptic errors

---

## Usage Examples

### Full Run (All Suppliers)
```powershell
python run_fast.py
```

**Expected time**: 30-60 minutes (with Gemini API) or 5-10 minutes (offline)

### Test Run (First 100 Rows)
```powershell
python run_fast.py --limit-rows 100
```

**Expected time**: 2-4 minutes (with API) or 20-40 seconds (offline)

### Custom Number of Workers
```powershell
# Use 4 cores (good for laptop with other apps running)
python run_fast.py --workers 4

# Use 16 cores (dedicated server)
python run_fast.py --workers 16
```

### Smaller Batches (More Frequent Checkpoints)
```powershell
python run_fast.py --batch-size 50
```

Checkpoint every 50 rows instead of 100 (better for unstable connections)

---

## Before You Start

### 1. Close Excel ‚ùó
The file must not be open in Excel (Windows file locking)

```powershell
# Check if file is open
Get-Process | Where-Object {$_.MainWindowTitle -like "*Frs.xlsx*"}
```

### 2. Ensure Dependencies Installed
```powershell
python -m pip install -r requirements.txt
```

### 3. Check API Key (Optional - for best accuracy)
```powershell
# Verify .env file exists
dir .env

# Check if key is loaded
python -c "import os; from dotenv import load_dotenv; load_dotenv(); print('API Key:', 'SET' if os.getenv('GEMINI_API_KEY') else 'NOT SET')"
```

**Without API key**: Uses offline mode (faster but less accurate)
**With API key**: Uses Gemini (slower but more accurate)

---

## During Execution

### What You'll See
```
[fast] Auto-detected 8 CPU cores
[fast] ‚úÖ Using temp checkpoint: C:\Users\...\Temp\sirene_pipeline_state.sqlite
[fast] ‚úÖ This avoids OneDrive file locking for 2-3x speedup

======================================================================
üöÄ FAST PIPELINE - Optimizations Active
======================================================================
‚úÖ Parallel workers: 8
‚úÖ Checkpoint location: Temp directory (off OneDrive)
‚úÖ Batch size: 100
‚úÖ Gemini API: Enabled
======================================================================

[pipeline] GEMINI_API_KEY found, using GeminiLLM in workers
[pipeline] Total to process: 2440 rows (skipped 0 already done)
[pipeline] Using 8 parallel workers

[pipeline] processed=100/2440 (4%) | rate=2.15 rows/sec | chunk_rate=8.5 rows/sec | ETA=18.1 mins
[pipeline] processed=200/2440 (8%) | rate=2.34 rows/sec | chunk_rate=9.1 rows/sec | ETA=15.9 mins
[pipeline] processed=300/2440 (12%) | rate=2.41 rows/sec | chunk_rate=8.8 rows/sec | ETA=14.8 mins
...
```

### Key Metrics
- **rate**: Overall rows/second (includes all overhead)
- **chunk_rate**: Current chunk processing speed (shows parallelization benefit)
- **ETA**: Estimated time remaining

### To Pause/Stop
Press `Ctrl+C` - progress is saved automatically

### To Resume
Just run the same command again - it skips already-processed rows

---

## After Completion

### Check Results
```powershell
# View output CSV
python -c "import pandas as pd; df=pd.read_csv('results_enriched.csv'); print(f'Total results: {len(df)}'); print(df.head())"
```

### Checkpoint Management
The script asks if you want to copy the checkpoint back to the project:
```
Copy checkpoint to project folder (state.sqlite)? (Y/n):
```

- **Y** (default): Copy to project for future reference
- **n**: Keep in temp (saves space, no OneDrive sync)

### View Statistics
```powershell
# Count by match method
python -c "import pandas as pd; df=pd.read_csv('results_enriched.csv'); print(df['match_method'].value_counts())"

# Show confidence score distribution
python -c "import pandas as pd; df=pd.read_csv('results_enriched.csv'); print(df['confidence_score'].describe())"
```

---

## Troubleshooting

### "Permission denied: Frs.xlsx"
**Problem**: Excel has the file open
**Solution**: Close Excel completely (check system tray)

### "database is locked"
**Problem**: Previous run didn't close cleanly
**Solution**: Delete temp checkpoint and try again
```powershell
del "$env:TEMP\sirene_pipeline_state.sqlite"
```

### Slow Performance Despite Parallelization
**Check**:
1. Is OneDrive syncing? (right-click OneDrive icon ‚Üí Pause syncing)
2. Is antivirus scanning? (add folder to exclusions)
3. Using Gemini free tier? (rate limited to 15/min)

### Out of Memory
**Solution**: Reduce workers
```powershell
python run_fast.py --workers 2
```

---

## Performance Comparison

### Sequential (Old Method)
```powershell
python run_pipeline.py run
```
**Time**: 3-7 hours for 2440 rows

### Parallel + Optimized (New Method)
```powershell
python run_fast.py
```
**Time**: 30-60 minutes for 2440 rows (with API)
**Time**: 5-10 minutes for 2440 rows (offline)

**Speedup**: 5-40x depending on setup

---

## Advanced Options

### Full Command Reference
```powershell
python run_fast.py --help
```

### Use OneDrive Checkpoint (Not Recommended)
```powershell
python run_fast.py --use-onedrive-checkpoint
```

Only use if you need the checkpoint in OneDrive for backup/sync

### Custom Output Location
```powershell
python run_fast.py --output-csv "C:\Results\enriched_$(Get-Date -Format 'yyyyMMdd').csv"
```

### Different Database
```powershell
python run_fast.py --duckdb-path "path\to\other\sirene.duckdb"
```

---

## FAQ

**Q: Can I run multiple instances at once?**
A: No - SQLite checkpoint is not designed for concurrent access

**Q: Will it use my API quota?**
A: Yes, but with caching it reduces redundant calls by 30-50%

**Q: Can I pause and resume later?**
A: Yes! Ctrl+C to pause, run same command to resume

**Q: What if my computer crashes?**
A: Checkpoint is saved every 100 rows, you'll only lose current batch

**Q: How do I switch between online/offline mode?**
A: Add/remove `GEMINI_API_KEY` from `.env` file

**Q: Is this safe for production?**
A: Yes - uses same logic as original, just faster

---

## Next Steps

1. **Run test**: `python run_fast.py --limit-rows 100`
2. **Check results**: Open `results_enriched.csv`
3. **Full run**: `python run_fast.py`
4. **Analyze**: Check match methods and confidence scores

For detailed performance analysis, see `PERFORMANCE_DEEP_ANALYSIS.md`
